# -*- coding: utf-8 -*-
"""ECIR_Updated_DP_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIqM6CfIY3Cuja5NCZ50yFKwpzLqcuYN
"""

import numpy as np
import pandas as pd
import numpy.random as npr
from scipy.linalg import sqrtm
from sklearn.metrics import jaccard_score
import os
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util
from google.colab import drive
drive.mount('/content/drive')


class AbstractMechanism:
    def __init__(self, m, epsilon=50, **kwargs):
        self.epsilon = epsilon
        self.m = m

    def noise_sampling(self):
        raise NotImplementedError("Subclasses should implement this!")

    def get_protected_vectors(self, embeddings):
        os.environ["OMP_NUM_THREADS"] = "1"
        noisy_embeddings = []
        for e in embeddings:
            noise = self.noise_sampling()
            noisy_embeddings.append(e + noise)
        return np.array(noisy_embeddings)

# CMP Mechanism class
class CMPMechanism(AbstractMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)

    def noise_sampling(self):
        N = npr.multivariate_normal(np.zeros(self.m), np.eye(self.m))
        X = N / np.sqrt(np.sum(N ** 2))
        Y = npr.gamma(self.m, 1 / self.epsilon)
        Z = Y * X
        return Z

# MM class
class MahalanobisMechanism(AbstractMechanism):
    def __init__(self, m, epsilon=50, embeddings=None, lam=1, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = lam
        self.emb_matrix = embeddings
        _, self.m = self.emb_matrix.shape
        cov_mat = np.cov(self.emb_matrix.T, ddof=0)
        self.sigma = cov_mat / np.mean(np.var(self.emb_matrix.T, axis=1))
        self.sigma_loc = sqrtm(self.lam * self.sigma + (1 - self.lam) * np.eye(self.m))

    def noise_sampling(self):
        N = npr.multivariate_normal(np.zeros(self.m), np.eye(self.m))
        X = N / np.sqrt(np.sum(N ** 2))
        X = np.matmul(self.sigma_loc, X)
        X = X / np.sqrt(np.sum(X ** 2))
        Y = npr.gamma(self.m, 1 / self.epsilon)
        Z = X * Y
        return Z

# VKM class
class VickreyMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = kwargs.get('lambda', 0.75)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)
        noisy_embeddings = []
        for e in embeddings:
            noisy_embeddings.append(e + self.noise_sampling())

        def euclidean_distance_matrix(x, y):
            x_expanded = x[:, np.newaxis, :]
            y_expanded = y[np.newaxis, :, :]
            return np.sqrt(np.sum((x_expanded - y_expanded) ** 2, axis=2))

        noisy_embeddings = np.array(noisy_embeddings)
        distance = euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)

        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings
# VKMM class
class VickreyMMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = kwargs.get('lambda', 0.75)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)
        noisy_embeddings = []
        for e in embeddings:
            noisy_embeddings.append(e + self.noise_sampling())

        def euclidean_distance_matrix(x, y):
            x_expanded = x[:, np.newaxis, :]
            y_expanded = y[np.newaxis, :, :]
            return np.sqrt(np.sum((x_expanded - y_expanded) ** 2, axis=2))

        noisy_embeddings = np.array(noisy_embeddings)
        distance = euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)

        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings

#VKCM class
class VickreyCMPMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, embeddings=None, lam=0.75, **kwargs):
        super().__init__(m, epsilon, embeddings=embeddings, lam=lam, **kwargs)
        self.cmp_mechanism = CMPMechanism(m, epsilon)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)

        noisy_embeddings = []
        for e in embeddings:
            noise = self.cmp_mechanism.noise_sampling()
            noisy_embeddings.append(e + noise)
        noisy_embeddings = np.array(noisy_embeddings)

        distance = self.euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)
        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings

    def euclidean_distance_matrix(self, x, y):
        x_expanded = x[:, np.newaxis, :]
        y_expanded = y[np.newaxis, :, :]
        return np.sqrt(np.sum((x_expanded - y_expanded) ** 2, axis=2))

#synthetic data generation based on embedding matrix
def generate_synthetic_data(n_samples, embedding_dim):
    embeddings = npr.randn(n_samples, embedding_dim)
    df = pd.DataFrame(embeddings, columns=[f"dim_{i}" for i in range(embedding_dim)])
    return df

#Query simulation function
def simulate_queries(n_queries, embedding_dim):
    queries = npr.randn(n_queries, embedding_dim)
    query_df = pd.DataFrame(queries, columns=[f"dim_{i}" for i in range(embedding_dim)])
    return query_df



def visualize_embeddings(original, protected_cmp, protected_mah, protected_vickrey):
    plt.figure(figsize=(15, 5))

    # Plot original embeddings
    plt.subplot(2, 2, 1)
    plt.scatter(original[:, 0], original[:, 1], alpha=0.5, c='blue', label='Original Embeddings')
    plt.title('Original Embeddings')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    #CMP protection
    plt.subplot(2, 2, 2)
    plt.scatter(protected_cmp[:, 0], protected_cmp[:, 1], alpha=0.5, c='red', label='Protected CMP')
    plt.title('Protected Embeddings (CMP)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    #Mahalanobis protection embeddings
    plt.subplot(2, 2, 3)
    plt.scatter(protected_mah[:, 0], protected_mah[:, 1], alpha=0.5, c='green', label='Protected Mahalanobis')
    plt.title('Protected Embeddings (Mahalanobis)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    #plt.tight_layout()
    #plt.show()

    # Vickrey protection embeddings
    plt.subplot(2, 2, 4)
    plt.scatter(protected_vickrey[:, 0], protected_vickrey[:, 1], alpha=0.5, c='yellow', label='Protected Vickrey')
    plt.title('Protected Embeddings (Vickrey)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    n_samples = 10
    embedding_dim = 300


    queries = [
        "What is the prognosis for endocarditis?",
        "What are the symptoms of diabetes?",
        "How to treat hypertension?",
        "What are the risk factors for heart disease?",
        "Is there a cure for Alzheimer's disease?"
    ]

    embeddings_df = generate_synthetic_data(n_samples, embedding_dim)
    embeddings = embeddings_df.to_numpy()

    model = SentenceTransformer('all-MiniLM-L6-v2')

    query_embeddings = model.encode(queries, convert_to_tensor=True).cpu().numpy()



    cmp_mechanism = CMPMechanism(m=embedding_dim)
    mahalanobis_mechanism = MahalanobisMechanism(m=embedding_dim, embeddings=embeddings)
    vickrey_mechanism = VickreyMechanism(m=embedding_dim, embeddings=embeddings)


    protected_cmp = cmp_mechanism.get_protected_vectors(embeddings)
    protected_mah = mahalanobis_mechanism.get_protected_vectors(embeddings)
    protected_vickrey = vickrey_mechanism.get_protected_vectors(embeddings)


    visualize_embeddings(embeddings, protected_cmp, protected_mah, protected_vickrey)


    print("Original Queries:\n", queries)
    print("Protected embeddings using CMP mechanism:\n", protected_cmp)
    print("\nProtected embeddings using Mahalanobis mechanism:\n", protected_mah)
    print("\nProtected embeddings using Vickrey mechanism:\n", protected_vickrey)


def load_glove_embeddings(glove_file_path):
    embeddings_dict = {}
    with open(glove_file_path, 'r', encoding="utf-8") as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], "float32")
            embeddings_dict[word] = vector
    return embeddings_dict


glove_file_path = "/content/drive/My Drive/glove.6B.300d.txt"
glove_embeddings = load_glove_embeddings(glove_file_path)


def find_closest_words(embedding, top_n=10):
    """Finds the closest words to a given embedding in the GloVe vocabulary."""
    distances = {word: np.linalg.norm(embedding - glove_embeddings[word])
                 for word in glove_embeddings if word in glove_embeddings}
    closest_words = sorted(distances, key=distances.get)[:top_n]
    return closest_words

def obfuscate_text(text, mechanism, glove_embeddings, embedding_dim=300):
    """Obfuscates text on a per-word basis using the given DP mechanism."""
    words = text.lower().split()

    embeddings = [glove_embeddings.get(word, np.zeros(embedding_dim)) for word in words]


    embeddings = [emb for emb in embeddings if emb.shape == (embedding_dim,)]

    if not embeddings:
        return ""

    embeddings = np.array(embeddings)


    if isinstance(mechanism, CMPMechanism):
        protected_embeddings = mechanism.get_protected_vectors(embeddings[:, :mechanism.m])
        protected_embeddings = np.concatenate([protected_embeddings, np.zeros((protected_embeddings.shape[0], embedding_dim - mechanism.m))], axis=1)
    else:

        projected_embeddings = embeddings[:, :mechanism.m] if mechanism.m != embedding_dim else embeddings
        protected_embeddings = mechanism.get_protected_vectors(projected_embeddings)
        protected_embeddings = np.concatenate([protected_embeddings, np.zeros((protected_embeddings.shape[0], embedding_dim - mechanism.m))], axis=1) if mechanism.m != embedding_dim else protected_embeddings


    obfuscated_words = [find_closest_words(emb, top_n=1)[0] for emb in protected_embeddings]
    return " ".join(obfuscated_words)

def calculate_similarities(original_query, protected_embeddings, mechanism, glove_embeddings, embedding_dim=300):
    """Calculates Jaccard and sentence similarities."""
    jaccard_similarities = []
    sentence_similarities = []

    model = SentenceTransformer('all-MiniLM-L6-v2')
    obfuscated_query = obfuscate_text(original_query, mechanism, glove_embeddings, embedding_dim)

    original_words = set(original_query.lower().split())
    obfuscated_words = set(obfuscated_query.lower().split())

    if original_words and obfuscated_words:
      jaccard_sim = len(original_words.intersection(obfuscated_words)) / len(original_words.union(obfuscated_words))
      jaccard_similarities.append(jaccard_sim)
    else:
      jaccard_similarities.append(np.nan)


    embedding1 = model.encode(original_query, convert_to_tensor=True)
    embedding2 = model.encode(obfuscated_query, convert_to_tensor=True)
    cosine_similarity_value = util.cos_sim(embedding1, embedding2).item()
    sentence_similarities.append(cosine_similarity_value)

    return jaccard_similarities, sentence_similarities

for i, original_query in enumerate(queries):
    print(f"\nOriginal Query {i + 1}:\n{original_query}")



    jaccard_cmp, sentence_cmp = calculate_similarities(original_query, protected_cmp, cmp_mechanism, glove_embeddings, embedding_dim) # Pass embedding_dim here
    print(f"CMP - Jaccard: {np.mean(jaccard_cmp):.4f}, Sentence: {np.mean(sentence_cmp):.4f}")


    jaccard_mah, sentence_mah = calculate_similarities(original_query, protected_mah, mahalanobis_mechanism, glove_embeddings)
    print(f"Mahalanobis - Jaccard: {np.mean(jaccard_mah):.4f}, Sentence: {np.mean(sentence_mah):.4f}")

    vickreym_mechanism = VickreyMMechanism(m=embedding_dim, embeddings=embeddings)
    jaccard_vickreym, sentence_vickreym = calculate_similarities(original_query, protected_cmp, vickreym_mechanism, glove_embeddings, embedding_dim)
    print(f"VickreyM - Jaccard: {np.mean(jaccard_vickreym):.4f}, Sentence: {np.mean(sentence_vickreym):.4f}")

    vickreycmp_mechanism = VickreyCMPMechanism(m=embedding_dim, embeddings=embeddings)
    jaccard_vickreycmp, sentence_vickreycmp = calculate_similarities(original_query, protected_cmp, vickreycmp_mechanism, glove_embeddings, embedding_dim)
    print(f"VickreyCMP - Jaccard: {np.mean(jaccard_vickreycmp):.4f}, Sentence: {np.mean(sentence_vickreycmp):.4f}")

    jaccard_vickrey, sentence_vickrey = calculate_similarities(original_query, protected_vickrey, vickrey_mechanism, glove_embeddings)
    print(f"Vickrey - Jaccard: {np.mean(jaccard_vickrey):.4f}, Sentence: {np.mean(sentence_vickrey):.4f}")