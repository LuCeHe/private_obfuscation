# -*- coding: utf-8 -*-
"""evaluate_goopir_dummy_query.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LwFp_VYDfAgd_3F_IWLmGLv2zqL-UFue
"""

# !pip install rank_bm25

from __future__ import division
from collections import Counter
import operator
import random
import numpy as np
from sklearn.metrics import average_precision_score, ndcg_score
from rank_bm25 import BM25Okapi
from transformers import BertTokenizer, BertModel
import torch

class goopir(object):
    def __init__(self, k=3):
        self._cff = 0.00001
        self._dict = {}
        self._sorted_list_dict = []
        self._k_num = k
        self.documents = self._generate_synthetic_health_data()
        self.bm25 = BM25Okapi(self.documents)
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.bert_model = BertModel.from_pretrained('bert-base-uncased')

    def _generate_synthetic_health_data(self):
        # Construct synthetic  documents
        synthetic_docs = [
            "patient confidentiality health records medication allergies",
            "symptoms of diabetes high blood sugar treatment options",
            "mental health awareness therapy depression anxiety",
            "health insurance coverage benefits exclusions",
            "nutrition diet obesity weight loss strategies",
            "vaccination schedule immunization side effects",
            "chronic pain management techniques rehabilitation",
            "screening tests early detection cancer health risks"
        ]
        return [doc.split(" ") for doc in synthetic_docs]

    def bogus_words(self, word):
        result = []
        interval = []
        k_num = self._k_num

        # Simulated frequency for the synthetic data
        freq = random.randint(1, 10)

        # Use random sample from synthetic documents for bogus words
        if word in self.documents:
            for i, doc in enumerate(self.documents):
                if word in doc:
                    result.append(' '.join(doc))
                    if len(result) >= k_num:
                        break
        else:
            random_sample = random.sample(self.documents, k=k_num)
            for doc in random_sample:
                result.append(' '.join(doc))

        return result

    def generate_fake_query(self, real_query):
        result = []
        rq = real_query.strip().lower().split(' ')
        for query_i in rq:
            if len(query_i) <= 2:
                continue
            result.extend(self.bogus_words(query_i))
        return random.sample(result, min(len(result), self._k_num))

    def bm25_score(self, query):
        tokenized_query = query.split(" ")
        scores = self.bm25.get_scores(tokenized_query)
        return scores

    def bert_score(self, query):
        # Use BERT to get the embedding
        inputs = self.tokenizer(query, return_tensors='pt')
        with torch.no_grad():
            outputs = self.bert_model(**inputs)

        # Return a 1-dimensional array instead of a 2-dimensional array and flatten the array to 1-d
        return outputs.last_hidden_state.mean(dim=1).numpy().flatten()

    def evaluate_metrics(self, original_query, fake_queries, relevance_labels):
        # BM25 Evaluation
        bm25_scores = self.bm25_score(original_query)

        bm25_ap = average_precision_score(relevance_labels, bm25_scores[:len(relevance_labels)])
        # Pass list of lists for bm25_scores
        bm25_ndcg_10 = ndcg_score([relevance_labels], [bm25_scores[:len(relevance_labels)]], k=10)
        # Pass list of lists for bm25_scores
        bm25_ndcg_100 = ndcg_score([relevance_labels], [bm25_scores[:len(relevance_labels)]], k=100)



        bert_embeddings = [self.bert_score(q) for q in fake_queries]
        original_embedding = self.bert_score(original_query)
        # Calculate similarities as a 1-dimensional array
        similarities = [np.dot(original_embedding, emb) for emb in bert_embeddings]
        bert_ap = average_precision_score(relevance_labels, similarities[:len(relevance_labels)])
        # Pass list of lists for bert scores
        bert_ndcg_10 = ndcg_score([relevance_labels], [similarities[:len(relevance_labels)]], k=10)
        # Pass list of lists for bert scores
        bert_ndcg_100 = ndcg_score([relevance_labels], [similarities[:len(relevance_labels)]], k=100)


        return {
            'BM25': {
                'AP': bm25_ap,
                'nDCG@10': bm25_ndcg_10,
                'nDCG@100': bm25_ndcg_100,
            },
            'BERT': {
                'AP': bert_ap,
                'nDCG@10': bert_ndcg_10,
                'nDCG@100': bert_ndcg_100,
            }
        }

if __name__ == "__main__":
    goopir_instance = goopir(5)

    #Sample Query
    queries = [
        "I want to know about patient confidentiality",
        "What are the symptoms of diabetes?",
        "How to manage mental health issues"
    ]

    for query in queries:
        fake_queries = goopir_instance.generate_fake_query(query)
        print(f"Original Query: {query} -> Fake Queries: {fake_queries}")


        # Create a relevance label where the first fake query is considered relevant
        relevance_labels = [1 if fq == fake_queries[0] else 0 for fq in fake_queries]

        # Matching the the number of fake_queries generated
        if len(relevance_labels) != len(fake_queries):
            print("Error: Relevance labels count does not match fake queries count.")
            continue

        evaluation_results = goopir_instance.evaluate_metrics(query, fake_queries, relevance_labels)
        print("==========Evaluation Results====================")
        print(f"BM25 AP: {evaluation_results['BM25']['AP']}")
        print(f"BM25 nDCG@10: {evaluation_results['BM25']['nDCG@10']}")
        print(f"BM25 nDCG@100: {evaluation_results['BM25']['nDCG@100']}")
        print(f"BERT AP: {evaluation_results['BERT']['AP']}")
        print(f"BERT nDCG@10: {evaluation_results['BERT']['nDCG@10']}")
        print(f"BERT nDCG@100: {evaluation_results['BERT']['nDCG@100']}")