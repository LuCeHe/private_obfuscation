# -*- coding: utf-8 -*-
"""candidate_query.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxRjU6ptvlB0RZ2hWQdMOYWfV_k30X2w
"""

import numpy as np
from rank_bm25 import BM25Okapi
from nltk.corpus import wordnet as wn
from itertools import combinations
from sklearn.metrics import ndcg_score
from sentence_transformers import SentenceTransformer, util
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer


nltk.download('wordnet')

#Define the corpus
corpus = [
    "What are the best elliptical trainers for home use?",
    "What are the benefits of an elliptical trainer compared to other fitness machines?",
    "Where can I get free diabetes education posters?",
    "I'm looking for nutrition and diet information for diabetics.",
    "Take me to the American Association of Diabetes Educators homepage."
]


target_documents = [0, 1, 2, 3, 4]

# Parameters for keyquery generation
k = 10

# Top-10 and Top-100 results
top_results = [10, 100]

model = SentenceTransformer('all-MiniLM-L6-v2')

# Vocabulary Expansion Function using WordNet
def expand_vocabulary(vocabulary):
    expanded_terms = set(vocabulary)
    for term in vocabulary:
        synsets = wn.synsets(term)
        # Synonym
        for syn in synsets:

            expanded_terms.add(syn.name().split('.')[0])
            # Hyponym
            for hyponym in syn.hyponyms():
                expanded_terms.add(hyponym.name().split('.')[0])
            # Hypernym
            for hypernym in syn.hypernyms():
                expanded_terms.add(hypernym.name().split('.')[0])
    return list(expanded_terms)


#Candidate Generation Approaches
def sliding_window_candidates(vocabulary, window_size=3):
    return [" ".join(vocabulary[i:i+window_size]) for i in range(len(vocabulary) - window_size + 1)]

def tfidf_candidates(corpus, vocabulary, max_features=10):
    vectorizer = TfidfVectorizer(vocabulary=vocabulary)
    tfidf_matrix = vectorizer.fit_transform(corpus)
    feature_names = vectorizer.get_feature_names_out()
    scores = np.mean(tfidf_matrix.toarray(), axis=0)
    top_terms = [feature_names[i] for i in np.argsort(scores)[-max_features:]]
    return [" ".join(combo) for combo in combinations(top_terms, 2)]

def noun_phrase_candidates(vocabulary):
    return [" ".join(combo) for combo in combinations(vocabulary, 2)]

#Apply HBC algorithm to filter based on similarity in practice
def apply_hbc(candidates):
    filtered_candidates = []
    seen_terms = set()
    for candidate in candidates:
        terms = set(candidate.split())
        if not seen_terms.intersection(terms):
            filtered_candidates.append(candidate)
            seen_terms.update(terms)
    return filtered_candidates

#BM25 Search
def bm25_search(corpus, query):
    tokenized_corpus = [doc.split(" ") for doc in corpus]
    bm25 = BM25Okapi(tokenized_corpus)
    tokenized_query = query.split(" ")
    scores = bm25.get_scores(tokenized_query)
    return np.argsort(scores)[::-1], scores

# Evaluation of candidate queries
def evaluate_candidates_table3(candidates, target_documents, label):
    avg_relevant_retrieved = {10: 0, 100: 0}
    precision_10 = 0
    ndcg_scores = {10: 0, 100: 0}
    recall_scores = {10: 0, 100: 0}
    num_queries = len(candidates)

    for query in candidates:
        indices, _ = bm25_search(corpus, query)
        for top_n in top_results:
            retrieved_documents = set(indices[:top_n])
            relevant_retrieved = len(retrieved_documents.intersection(target_documents))
            avg_relevant_retrieved[top_n] += relevant_retrieved


            if top_n == 10:
                precision_10 += relevant_retrieved / min(len(retrieved_documents), k)


            recall_scores[top_n] += relevant_retrieved / len(target_documents)


            relevance_scores = [1 if i in target_documents else 0 for i in range(len(corpus))]
            retrieved_relevance = [1 if idx in target_documents else 0 for idx in indices[:top_n]]
            ndcg_scores[top_n] += ndcg_score([relevance_scores], [retrieved_relevance])

    # Averaging the metrics over the number of queries
    for top_n in top_results:
        avg_relevant_retrieved[top_n] /= num_queries
        recall_scores[top_n] /= num_queries
        ndcg_scores[top_n] /= num_queries
    precision_10 /= num_queries


    print(f"{label}:")
    print(f"  Number of Candidate Queries: {num_queries}")
    print(f"  Avg Relevant (Top-10): {avg_relevant_retrieved[10]:.2f}, "
          f"Avg Relevant (Top-100): {avg_relevant_retrieved[100]:.2f}")
    print(f"  Precision@10: {precision_10:.4f}")
    print(f"  Recall@10: {recall_scores[10]:.4f}, Recall@100: {recall_scores[100]:.4f}")
    print(f"  nDCG@10: {ndcg_scores[10]:.4f}, nDCG@100: {ndcg_scores[100]:.4f}\n")


def evaluate_candidates_table2(candidates, target_documents, label):
    recall_total = 0
    num_queries = len(candidates)
    for query in candidates:
        indices, _ = bm25_search(corpus, query)
        retrieved_documents = set(indices[:k])
        relevant_retrieved = len(retrieved_documents.intersection(target_documents))
        recall_total += relevant_retrieved / len(target_documents)  # Recall for each query

    avg_recall = recall_total / num_queries if num_queries > 0 else 0


    print(f"{label}:")
    print(f"  Number of Candidate Queries: {num_queries}")
    print(f"  Avg Recall (Quality of Candidates): {avg_recall:.4f}\n")

#Generate expanded vocabulary
original_vocabulary = ["elliptical", "trainer", "diabetes", "nutrition", "education", "fitness", "posters"]
expanded_vocabulary = expand_vocabulary(original_vocabulary)

# Generate candidate queries based on each approach with expanded vocabulary
# Sliding Window
candidates_sliding = sliding_window_candidates(expanded_vocabulary)


candidates_tfidf = tfidf_candidates(corpus, expanded_vocabulary)

#Noun phrase extraction
candidates_noun_phrase = noun_phrase_candidates(expanded_vocabulary)

# Apply HBC
candidates_sliding_hbc = apply_hbc(candidates_sliding)
candidates_tfidf_hbc = apply_hbc(candidates_tfidf)
candidates_noun_phrase_hbc = apply_hbc(candidates_noun_phrase)

#Number and quality (Recall) of candidate queries
print("\n Candidate query quality and quantity")
for candidates, label in zip(
    [candidates_sliding, candidates_sliding_hbc, candidates_tfidf, candidates_tfidf_hbc, candidates_noun_phrase, candidates_noun_phrase_hbc],
    ["Sliding Window", "Sliding Window + HBC", "TF-IDF", "TF-IDF + HBC", "Noun Phrase", "Noun Phrase + HBC"]
):
    evaluate_candidates_table2(candidates, target_documents, label)


print("\nCandidate query evaluation metrics")
for candidates, label in zip(
    [candidates_sliding, candidates_sliding_hbc, candidates_tfidf, candidates_tfidf_hbc, candidates_noun_phrase, candidates_noun_phrase_hbc],
    ["Sliding Window", "Sliding Window + HBC", "TF-IDF", "TF-IDF + HBC", "Noun Phrase", "Noun Phrase + HBC"]
):
    evaluate_candidates_table3(candidates, target_documents, label)