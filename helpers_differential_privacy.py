# -*- coding: utf-8 -*-
"""ECIR_Updated_DP_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SIqM6CfIY3Cuja5NCZ50yFKwpzLqcuYN
"""

import numpy as np
import pandas as pd
import numpy.random as npr
from scipy.linalg import sqrtm
import os
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer, util


def euclidean_distance_matrix(x, y):
    x_expanded = x[:, np.newaxis, :]
    y_expanded = y[np.newaxis, :, :]
    return np.sqrt(np.sum((x_expanded - y_expanded) ** 2, axis=2))


class AbstractMechanism:
    def __init__(self, m, epsilon=50, **kwargs):
        self.epsilon = epsilon
        self.m = m

    def noise_sampling(self):
        raise NotImplementedError("Subclasses should implement this!")

    def get_protected_vectors(self, embeddings):
        os.environ["OMP_NUM_THREADS"] = "1"
        noisy_embeddings = []
        for e in embeddings:
            noise = self.noise_sampling()
            noisy_embeddings.append(e + noise)
        return np.array(noisy_embeddings)


# CMP Mechanism class
class CMPMechanism(AbstractMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)

    def noise_sampling(self):
        N = npr.multivariate_normal(np.zeros(self.m), np.eye(self.m))
        X = N / np.sqrt(np.sum(N ** 2))
        Y = npr.gamma(self.m, 1 / self.epsilon)
        Z = Y * X
        return Z


# MM class
class MahalanobisMechanism(AbstractMechanism):
    def __init__(self, m, epsilon=50, embeddings=None, lam=1, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = lam
        self.emb_matrix = embeddings
        _, self.m = self.emb_matrix.shape
        cov_mat = np.cov(self.emb_matrix.T, ddof=0)
        self.sigma = cov_mat / np.mean(np.var(self.emb_matrix.T, axis=1))
        self.sigma_loc = sqrtm(self.lam * self.sigma + (1 - self.lam) * np.eye(self.m))

    def noise_sampling(self):
        N = npr.multivariate_normal(np.zeros(self.m), np.eye(self.m))
        X = N / np.sqrt(np.sum(N ** 2))
        X = np.matmul(self.sigma_loc, X)
        X = X / np.sqrt(np.sum(X ** 2))
        Y = npr.gamma(self.m, 1 / self.epsilon)
        Z = X * Y
        return Z


# VKM class
class VickreyMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = kwargs.get('lambda', 0.75)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)
        noisy_embeddings = []
        for e in embeddings:
            noisy_embeddings.append(e + self.noise_sampling())

        noisy_embeddings = np.array(noisy_embeddings)
        distance = euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)

        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (
                self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings


# VKMM class
class VickreyMMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, **kwargs):
        super().__init__(m, epsilon, **kwargs)
        self.lam = kwargs.get('lambda', 0.75)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)
        noisy_embeddings = []
        for e in embeddings:
            noisy_embeddings.append(e + self.noise_sampling())

        noisy_embeddings = np.array(noisy_embeddings)
        distance = euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)

        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (
                self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings


# VKCM class
class VickreyCMPMechanism(MahalanobisMechanism):
    def __init__(self, m, epsilon=50, embeddings=None, lam=0.75, **kwargs):
        super().__init__(m, epsilon, embeddings=embeddings, lam=lam, **kwargs)
        self.cmp_mechanism = CMPMechanism(m, epsilon)

    def get_protected_vectors(self, embeddings):
        n_words = len(embeddings)

        noisy_embeddings = []
        for e in embeddings:
            noise = self.cmp_mechanism.noise_sampling()
            noisy_embeddings.append(e + noise)
        noisy_embeddings = np.array(noisy_embeddings)

        distance = euclidean_distance_matrix(noisy_embeddings, self.emb_matrix)
        closest = np.argpartition(distance, 2, axis=1)[:, :2]
        dist_to_closest = distance[np.tile(np.arange(n_words).reshape(-1, 1), 2), closest]

        p = ((1 - self.lam) * dist_to_closest[:, 1]) / (
                self.lam * dist_to_closest[:, 0] + (1 - self.lam) * dist_to_closest[:, 1])

        vickrey_choice = np.array([npr.choice(2, p=[p[w], 1 - p[w]]) for w in range(n_words)])
        noisy_embeddings = self.emb_matrix[closest[np.arange(n_words), vickrey_choice]]

        return noisy_embeddings


# synthetic data generation based on embedding matrix
# def generate_synthetic_data(n_samples, embedding_dim):
#     embeddings = npr.randn(n_samples, embedding_dim)
#     df = pd.DataFrame(embeddings, columns=[f"dim_{i}" for i in range(embedding_dim)])
#     return df


# Query simulation function
def simulate_queries(n_queries, embedding_dim):
    queries = npr.randn(n_queries, embedding_dim)
    query_df = pd.DataFrame(queries, columns=[f"dim_{i}" for i in range(embedding_dim)])
    return query_df


def visualize_embeddings(original, protected_cmp, protected_mah, protected_vickrey):
    plt.figure(figsize=(15, 5))

    # Plot original embeddings
    plt.subplot(2, 2, 1)
    plt.scatter(original[:, 0], original[:, 1], alpha=0.5, c='blue', label='Original Embeddings')
    plt.title('Original Embeddings')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    # CMP protection
    plt.subplot(2, 2, 2)
    plt.scatter(protected_cmp[:, 0], protected_cmp[:, 1], alpha=0.5, c='red', label='Protected CMP')
    plt.title('Protected Embeddings (CMP)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    # Mahalanobis protection embeddings
    plt.subplot(2, 2, 3)
    plt.scatter(protected_mah[:, 0], protected_mah[:, 1], alpha=0.5, c='green', label='Protected Mahalanobis')
    plt.title('Protected Embeddings (Mahalanobis)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    # plt.tight_layout()
    # plt.show()

    # Vickrey protection embeddings
    plt.subplot(2, 2, 4)
    plt.scatter(protected_vickrey[:, 0], protected_vickrey[:, 1], alpha=0.5, c='yellow', label='Protected Vickrey')
    plt.title('Protected Embeddings (Vickrey)')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()


def load_glove_embeddings(gensim_name='glove-twitter-25'):
    import gensim.downloader as api
    embeddings_dict = api.load(gensim_name)

    return embeddings_dict


def find_closest_words(embedding, top_n=10):
    """Finds the closest words to a given embedding in the GloVe vocabulary."""
    distances = {
        word: np.linalg.norm(embedding - glove_embeddings[word])
        for word in glove_embeddings
    }
    closest_words = sorted(distances, key=distances.get)[:top_n]
    return closest_words


def obfuscate_text(text, mechanism, glove_embeddings):
    embedding_dim = glove_embeddings.vector_size
    """Obfuscates text on a per-word basis using the given DP mechanism."""
    words = text.lower().split()

    embeddings = [glove_embeddings.get(word, np.zeros(embedding_dim)) for word in words]

    if not embeddings:
        return ""

    embeddings = np.array(embeddings)

    protected_embeddings = mechanism.get_protected_vectors(embeddings)
    print('protected_embeddings.shape', protected_embeddings.shape)

    obfuscated_words = [find_closest_words(emb, top_n=1)[0] for emb in protected_embeddings]
    return " ".join(obfuscated_words)


def calculate_similarities(original_query, obfuscated_query, semantic_model):
    """Calculates Jaccard and sentence similarities."""

    original_words = set(original_query.lower().split())
    obfuscated_words = set(obfuscated_query.lower().split())

    if original_words and obfuscated_words:
        jaccard_similarity = len(original_words.intersection(obfuscated_words)) / len(original_words.union(obfuscated_words))
    else:
        jaccard_similarity = np.nan

    embedding1 = semantic_model.encode(original_query, convert_to_tensor=True)
    embedding2 = semantic_model.encode(obfuscated_query, convert_to_tensor=True)
    semantic_similarity = util.cos_sim(embedding1, embedding2).item()

    return jaccard_similarity, semantic_similarity


if __name__ == "__main__":

    glove_embeddings = load_glove_embeddings()
    embedding_dim = glove_embeddings.vector_size

    queries = [
        "What is the prognosis for endocarditis?",
        "What are the symptoms of diabetes?",
        "How to treat hypertension?",
        "What are the risk factors for heart disease?",
        "Is there a cure for Alzheimer's disease?"
    ]

    glove_matrix = np.array([glove_embeddings[word] for word in glove_embeddings.index_to_key])

    epsilon = 50
    mechs = {
        "CMP": CMPMechanism(m=embedding_dim, epsilon=epsilon),
        "Mahalanobis": MahalanobisMechanism(m=embedding_dim, epsilon=epsilon, embeddings=glove_matrix),
        "Vickrey": VickreyMechanism(m=embedding_dim, epsilon=epsilon, embeddings=glove_matrix),
        "VickreyM": VickreyMMechanism(m=embedding_dim, epsilon=epsilon, embeddings=glove_matrix),
        "VickreyCMP": VickreyCMPMechanism(m=embedding_dim, epsilon=epsilon, embeddings=glove_matrix)
    }
    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')

    for i, original_query in enumerate(queries):
        print(f"\nOriginal Query {i + 1}: {original_query}")

        for mech_name, mech in mechs.items():
            print( '   Mechanism:', mech_name)
            obfuscated_query = obfuscate_text(original_query, mech, glove_embeddings)
            print(f"   Obfuscated Query: {obfuscated_query}")
            jaccard_sim, semantic_sim = calculate_similarities(original_query, obfuscated_query, semantic_model)
            print(f"{mech_name} - Jaccard: {jaccard_sim:.4f}, Semantic: {semantic_sim:.4f}")
