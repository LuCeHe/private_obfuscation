# -*- coding: utf-8 -*-
"""evaluate_nispp_dummy_query.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mk4YbbrWa1SexEY90oVTtgXjL5N_5x1u
"""

from __future__ import division
import random
import math
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from collections import defaultdict

class nispp(object):
    def __init__(self, e=0.5, user_queries=None):
        self.e = e
        self.lamda = 0
        self.w_sum = 0
        self.word_data = {}
        self.reverse_word_data = {}
        self.user_queries = user_queries if user_queries is not None else []
        self.bm25_vectorizer = TfidfVectorizer()
        self.bm25_model = None
        self.bert_model = SentenceTransformer('all-MiniLM-L6-v2')
        #Generate synthetic dataset
        self.generate_synthetic_data()
        self.calculate_user_query_probability()
        self.calculate_fake_query_probability()

    def generate_synthetic_data(self):
        #generate 1000 syntnetic queries
        self.user_queries = [f"query {i} for testing" for i in range(1, 1001)]
        # Randomly assign frequencies to queries
        self.word_data = {query: random.randint(1, 1000) for query in self.user_queries}
        self.reverse_word_data = {i: query for i, query in enumerate(self.user_queries)}

    def set_e(self, new_e):
        self.e = new_e

    def calculate_user_query_probability(self):
        t_sum_1 = 0
        t_sum_2 = 0
        for key in self.word_data.keys():
            self.w_sum += self.word_data[key]
        for key in self.word_data.keys():
            u = self.word_data[key] / self.w_sum
            t_sum_1 += (7 - 2 * u) / 6
            t_sum_2 += 2 / (u * u - u)
        self.lamda = (1 - t_sum_1) / t_sum_2

    def calculate_fake_query_probability(self):
        # For simplicity, using the same word_data for reverse probabilities
        self.reverse_word_data = {k: v for v, k in self.word_data.items()}

    def calculate_realp_map_fackp(self, r_p):
        f_p = ((7 - 2 * r_p) / 6) + (1 / (1 - self.e)) * self.lamda / (r_p * r_p - r_p)
        return f_p

    def generate_fake_queries(self, real_queries):
        fake_queries = []
        for real_query in real_queries:
            if real_query in self.user_queries:
                fake_queries.extend(random.sample(self.user_queries, math.floor(1 / self.e)))
            else:
                fake_queries.extend(random.sample(self.user_queries, math.floor(1 / self.e)))
        return list(set(fake_queries))  # Return unique fake queries

    def bm25(self, queries):
        # Calculate BM25 scores for multiple queries against user queries
        if self.bm25_model is None:
            self.bm25_model = self.bm25_vectorizer.fit_transform(self.user_queries)

        #Generate BM25 scores for each query
        results = {}
        for query in queries:
            query_vector = self.bm25_vectorizer.transform([query])
            scores = np.dot(self.bm25_model, query_vector.T).toarray().flatten()
            top_indices = np.argsort(scores)[-3:][::-1]
            results[query] = [self.user_queries[i] for i in top_indices]
        return results

    def dense_model(self, queries):
        # Calculate BERT embeddings for multiple queries and find the most similar queries
        user_embeddings = self.bert_model.encode(self.user_queries)
        results = {}

        for query in queries:
            query_embedding = self.bert_model.encode(query)
            # Compute cosine similarities
            similarities = np.dot(user_embeddings, query_embedding) / (
                np.linalg.norm(user_embeddings, axis=1) * np.linalg.norm(query_embedding))
            top_indices = np.argsort(similarities)[-3:][::-1]
            results[query] = [self.user_queries[i] for i in top_indices]
        return results

    def measure_similarity(self, original_queries, fake_queries):
        # Calculate similarity scores between original queries and fake queries
        similarities_results = {}
        for original_query in original_queries:
            original_embedding = self.bert_model.encode(original_query)
            fake_embeddings = self.bert_model.encode(fake_queries)
            # Compute cosine similarities
            similarities = np.dot(fake_embeddings, original_embedding) / (
                np.linalg.norm(fake_embeddings, axis=1) * np.linalg.norm(original_embedding))
            similarities_results[original_query] = similarities
        return similarities_results

if __name__ == '__main__':
    queries = ['query 10 for testing', 'query 20 for testing']  # Sample multiple queries
    model = nispp()

    # Generate fake queries for all provided queries
    fake_queries = model.generate_fake_queries(queries)

    print("BM25 Results:")
    bm25_results = model.bm25(queries)
    for query, results in bm25_results.items():
        print(f"Query: '{query}', Top BM25 Results: {results}")

    print("\nBERT Results:")
    bert_results = model.dense_model(queries)
    for query, results in bert_results.items():
        print(f"Query: '{query}', Top BERT Results: {results}")

    print("\nGenerated Fake Queries:")
    print(fake_queries)

    # Measure similarity between original queries and fake queries
    similarities = model.measure_similarity(queries, fake_queries)

    print("\nSimilarity Scores between Original and Fake Queries:")
    for original_query, similarity_scores in similarities.items():
        for fake_query, similarity in zip(fake_queries, similarity_scores):
            print(f"Original Query: '{original_query}', Fake Query: '{fake_query}', Similarity Score: {similarity:.4f}")

!pip install sentence_transformers